# coding=utf-8
"""Latent Dirichlet allocation using collapsed Gibbs sampling"""
from __future__ import absolute_import, division, print_function, unicode_literals
import logging
import sys

import numpy as np
import scipy.special

import lda._lda
import lda.utils

PY2 = sys.version_info[0] == 2
if PY2:
    from itertools import izip as zip

logger = logging.getLogger('lda')


class LDA:
    """Latent Dirichlet allocation using collapsed Gibbs sampling

    Parameters
    ----------
    n_topics : int
        Number of topics

    n_iter : int, default 1000
        Number of sampling iterations

    alpha : float, default 0.1
        Dirichlet parameter for distribution over topics

    eta : float, default 0.01
        Dirichlet parameter for distribution over words

    random_state : int or RandomState, optional
        The generator used for the initial topics.

    Attributes
    ----------
    `components_` : array, shape = [n_topics, n_features]
        Point estimate of the topic-word distributions (Phi in literature)
    `topic_word_` :
        Alias for `components_`
    `nzw_` : array, shape = [n_topics, n_features]
        Matrix of counts recording topic-word assignments in final iteration.
    `ndz_` : array, shape = [n_samples, n_features]
        Matrix of counts recording document-topic assignments in final iteration.
    `doc_topic_` : array, shape = [n_samples, n_features]
        Point estimate of the document-topic distributions (Theta in literature)
    `nz_` : array, shape = [n_topics]
        Array of topic assignment counts in final iteration.

    Examples
    --------
    >>> import numpy
    >>> X = numpy.array([[1,1], [2, 1], [3, 1], [4, 1], [5, 8], [6, 1]])
    >>> import lda
    >>> model = lda.LDA(n_topics=2, random_state=0, n_iter=100)
    >>> model.fit(X) #doctest: +ELLIPSIS +NORMALIZE_WHITESPACE
    LDA(alpha=...
    >>> model.components_
    array([[ 0.85714286,  0.14285714],
           [ 0.45      ,  0.55      ]])
    >>> model.loglikelihood() #doctest: +ELLIPSIS
    -40.395...

    Note
    ----
    Although this module uses code written in C (generated by Cython) in its
    implementation of LDA, sampling is considerably slower than Java (MALLET_)
    and C implementations (`HCA <http://www.mloss.org/software/view/527/>`_).
    Consider using different software if you are working a large dataset.

    References
    ----------
    Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent Dirichlet
    Allocation." Journal of Machine Learning Research 3 (2003): 993–1022.

    Griffiths, Thomas L., and Mark Steyvers. "Finding Scientific Topics."
    Proceedings of the National Academy of Sciences 101 (2004): 5228–5235.
    doi:10.1073/pnas.0307752101.

    Wallach, Hanna, David Mimno, and Andrew McCallum. "Rethinking LDA: Why
    Priors Matter." In Advances in Neural Information Processing Systems 22,
    edited by Y.  Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams, and A.
    Culotta, 1973–1981, 2009.
    """

    def __init__(self, n_topics=None, n_iter=1000, alpha=0.1, eta=0.01, random_state=None, refresh=10):
        self.n_topics = n_topics
        self.n_iter = n_iter
        self.alpha = alpha
        self.eta = eta
        self.random_state = random_state
        self.refresh = refresh

        # random numbers that are reused
        rng = lda.utils.check_random_state(random_state)
        self._rands = rng.rand(1000)

    def fit(self, X, y=None):
        """Fit the model with X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Training data, where n_samples in the number of samples
            and n_features is the number of features.

        Returns
        -------
        self : object
            Returns the instance itself.
        """
        self._fit(X)
        return self

    def _fit(self, X):
        """Fit the model to the data X.

        Parameters
        ----------
        X: array-like, shape (n_samples, n_features)
            Training vector, where n_samples in the number of samples and
            n_features is the number of features.
        """
        random_state = lda.utils.check_random_state(self.random_state)
        X = np.atleast_2d(X).astype(np.float64)
        self._initialize(X, random_state)
        for it in range(self.n_iter):
            if it % self.refresh == 0:
                self._print_status(it)
            self._sample_topics(random_state)
        self._print_status(self.n_iter)
        self.components_ = self.nzw_ + self.eta
        self.components_ /= np.sum(self.components_, axis=1, keepdims=True)
        self.topic_word_ = self.components_
        self.doc_topic_ = self.ndz_ + self.alpha
        self.doc_topic_ /= np.sum(self.doc_topic_, axis=1, keepdims=True)

        # delete attributes no longer needed after fitting to save memory and reduce clutter
        del self.WS
        del self.DS
        del self.ZS
        return self

    def _print_status(self, iter):
        ll = self.loglikelihood()
        N = len(self.WS)
        logger.info("<{}> log likelihood: {:.0f}, log perplexity: {:.4f}".format(iter, ll, -1 * ll / N))

    def _initialize(self, X, random_state):
        D, W = X.shape
        N = int(np.sum(X))
        random_state = lda.utils.check_random_state(self.random_state)
        n_topics = self.n_topics
        n_iter = self.n_iter
        logger.info("n_documents: {}".format(D))
        logger.info("vocab_size: {}".format(W))
        logger.info("n_words: {}".format(N))
        logger.info("n_topics: {}".format(n_topics))
        logger.info("n_iter: {}".format(n_iter))

        self.nzw_ = np.zeros((n_topics, W), dtype=np.intc)
        self.ndz_ = np.zeros((D, n_topics), dtype=np.intc)
        self.nz_ = np.zeros(n_topics, dtype=np.intc)

        self.WS, self.DS = lda.utils.matrix_to_lists(X)
        self.ZS = np.zeros_like(self.WS, dtype=np.intc)
        for i, (w, d) in enumerate(zip(self.WS, self.DS)):
            # random initialization
            # FIXME: initialization could occur elsewhere
            z_new = random_state.randint(n_topics)
            self.ZS[i] = z_new
            self.ndz_[d, z_new] += 1
            self.nzw_[z_new, w] += 1
            self.nz_[z_new] += 1

    def loglikelihood(self):
        nzw, ndz, nz = self.nzw_, self.ndz_, self.nz_
        alpha = self.alpha
        eta = self.eta
        return self._loglikelihood(nzw, ndz, nz, alpha, eta)

    # TODO: Cythonize _loglikelihood
    # TODO: Use John Cook's version of lgamma rather than scipy
    @staticmethod
    def _loglikelihood(nzw, ndz, nz, alpha, eta):
        """
        Calculate complete log likelihood, log p(w,z)

        log p(w,z) = log p(w|z) + log p(z)
        """
        D, n_topics = ndz.shape
        vocab_size = nzw.shape[1]
        nd = np.sum(ndz, axis=1)

        ll = 0.0

        # calculate log p(w|z)
        gammaln_eta = scipy.special.gammaln(eta)
        gammaln_alpha = scipy.special.gammaln(alpha)

        ll += n_topics * scipy.special.gammaln(eta * vocab_size)
        for k in range(n_topics):
            ll -= scipy.special.gammaln(eta * vocab_size + nz[k])
            for w in range(vocab_size):
                # if nzw[k, w] == 0 addition and subtraction cancel out
                if nzw[k, w] > 0:
                    ll += scipy.special.gammaln(eta + nzw[k, w]) - gammaln_eta

        # calculate log p(z)
        for d in range(D):
            ll += scipy.special.gammaln(alpha * n_topics) - scipy.special.gammaln(alpha * n_topics + nd[d])
            for k in range(n_topics):
                if ndz[d, k] > 0:
                    ll += scipy.special.gammaln(alpha + ndz[d, k]) - gammaln_alpha
        return ll

    def _sample_topics(self, random_state):
        random_state = lda.utils.check_random_state(self.random_state)
        rands = self._rands
        # every shuffling is the same permutation; but every time the initial self._rands is different
        random_state.shuffle(rands)
        n_topics, vocab_size = self.nzw_.shape
        alpha = np.repeat(self.alpha, n_topics).astype(np.float64)
        eta = np.repeat(self.eta, vocab_size).astype(np.float64)
        lda._lda._sample_topics(self.WS, self.DS, self.ZS, self.nzw_, self.ndz_, self.nz_, alpha, eta, rands)

    def transform(self, X, y=None):
        """Transform the data X according to previously fitted model

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            New data, where n_samples in the number of samples
            and n_features is the number of features.

        Returns
        -------
        doc_topic : array-like, shape (n_samples, n_topics)
            Point estimate of the document-topic distributions

        """
        raise NotImplementedError

    def fit_transform(self, X, y=None):
        """Apply dimensionality reduction on X

        Parameters
        ----------
        X : array-like, shape (n_samples, n_features)
            New data, where n_samples in the number of samples
            and n_features is the number of features.

        Returns
        -------
        doc_topic : array-like, shape (n_samples, n_topics)
            Point estimate of the document-topic distributions

        """
        self._fit(np.atleast_2d(X))
        return self.doc_topic_
